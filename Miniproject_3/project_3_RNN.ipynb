{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "from HMM import unsupervised_HMM\n",
    "from HMM_helper import (\n",
    "    text_to_wordcloud,\n",
    "    states_to_wordclouds,\n",
    "    parse_observations,\n",
    "    sample_sentence,\n",
    "    visualize_sparsities,\n",
    "    animate_emission,\n",
    "    obs_map_reverser)\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import LambdaCallback\n",
    "import random\n",
    "import sys\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open and read file\n",
    "file = open('data/shakespeare.txt', 'r')\n",
    "text = file.read()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split by sonnet and get rid of sonnet numbering\n",
    "sonnets2 = text.split('\\n\\n\\n')\n",
    "sonnets2 = [sonnet[(sonnet.find('\\n')+1):] for sonnet in sonnets2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate 40 character sentences for training, plus y vector of next characters\n",
    "sentences = []\n",
    "next_chars = []\n",
    "\n",
    "#Length 40, step size is 3\n",
    "maxlen = 40\n",
    "step_size = 3\n",
    "for sonnet in sonnets2:\n",
    "    for i in range(0, len(sonnet)-max_len,step_size):\n",
    "        sentences.append(sonnet[i:(i+max_len)])\n",
    "        next_chars.append(sonnet[i+max_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the unique characters and create a forward and backward mapping to indices\n",
    "raw_text = ' '.join(sonnets2)\n",
    "chars = sorted(list(set(raw_text)))\n",
    "mapping = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find out the vocabulary size\n",
    "vocab_size = len(mapping)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert sentences to indices\n",
    "encoded = list()\n",
    "for line in sentences:\n",
    "\t# integer encode line\n",
    "\tencoded_seq = [mapping[char] for char in line]\n",
    "\t# store\n",
    "\tencoded.append(encoded_seq)\n",
    "\n",
    "#Convert next characters to indices\n",
    "encoded_nextchar = [mapping[char] for char in next_chars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One hot encode indices\n",
    "sequences = [to_categorical(x, num_classes=vocab_size) for x in encoded]\n",
    "X = np.array(sequences)\n",
    "y = to_categorical(encoded_nextchar, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (None, 150)               127200    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 61)                9211      \n",
      "=================================================================\n",
      "Total params: 136,411\n",
      "Trainable params: 136,411\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(150, input_shape = (X.shape[1], X.shape[2])))\n",
    "model.add(Dense(vocab_size, activation = 'softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " - 159s - loss: 0.8326 - accuracy: 0.7425\n",
      "Epoch 2/100\n",
      " - 156s - loss: 0.7922 - accuracy: 0.7570\n",
      "Epoch 3/100\n",
      " - 146s - loss: 0.7602 - accuracy: 0.7659\n",
      "Epoch 4/100\n",
      " - 168s - loss: 0.7308 - accuracy: 0.7781\n",
      "Epoch 5/100\n",
      " - 184s - loss: 0.7059 - accuracy: 0.7855\n",
      "Epoch 6/100\n",
      " - 188s - loss: 0.6789 - accuracy: 0.7923\n",
      "Epoch 7/100\n",
      " - 184s - loss: 0.6498 - accuracy: 0.8062\n",
      "Epoch 8/100\n",
      " - 141s - loss: 0.6567 - accuracy: 0.8008\n",
      "Epoch 9/100\n",
      " - 142s - loss: 0.5976 - accuracy: 0.8210\n",
      "Epoch 10/100\n",
      " - 159s - loss: 0.5724 - accuracy: 0.8318\n",
      "Epoch 11/100\n",
      " - 178s - loss: 0.5598 - accuracy: 0.8348\n",
      "Epoch 12/100\n",
      " - 154s - loss: 0.5464 - accuracy: 0.8374\n",
      "Epoch 13/100\n",
      " - 145s - loss: 0.5204 - accuracy: 0.8479\n",
      "Epoch 14/100\n",
      " - 158s - loss: 0.5080 - accuracy: 0.8519\n",
      "Epoch 15/100\n",
      " - 158s - loss: 0.4965 - accuracy: 0.8536\n",
      "Epoch 16/100\n",
      " - 140s - loss: 0.4753 - accuracy: 0.8597\n",
      "Epoch 17/100\n",
      " - 144s - loss: 0.4595 - accuracy: 0.8658\n",
      "Epoch 18/100\n",
      " - 145s - loss: 0.4447 - accuracy: 0.8719\n",
      "Epoch 19/100\n",
      " - 155s - loss: 0.4408 - accuracy: 0.8742\n",
      "Epoch 20/100\n",
      " - 145s - loss: 0.4155 - accuracy: 0.8804\n",
      "Epoch 21/100\n",
      " - 143s - loss: 0.4080 - accuracy: 0.8827\n",
      "Epoch 22/100\n",
      " - 155s - loss: 0.3949 - accuracy: 0.8875\n",
      "Epoch 23/100\n",
      " - 156s - loss: 0.3886 - accuracy: 0.8889\n",
      "Epoch 24/100\n",
      " - 147s - loss: 0.3689 - accuracy: 0.8959\n",
      "Epoch 25/100\n",
      " - 149s - loss: 0.3653 - accuracy: 0.8974\n",
      "Epoch 26/100\n",
      " - 142s - loss: 0.3579 - accuracy: 0.9000\n",
      "Epoch 27/100\n",
      " - 141s - loss: 0.3386 - accuracy: 0.9053\n",
      "Epoch 28/100\n",
      " - 149s - loss: 0.3410 - accuracy: 0.9031\n",
      "Epoch 29/100\n",
      " - 153s - loss: 0.3210 - accuracy: 0.9120\n",
      "Epoch 30/100\n",
      " - 138s - loss: 0.3203 - accuracy: 0.9100\n",
      "Epoch 31/100\n",
      " - 140s - loss: 0.3108 - accuracy: 0.9127\n",
      "Epoch 32/100\n",
      " - 136s - loss: 0.3159 - accuracy: 0.9106\n",
      "Epoch 33/100\n",
      " - 138s - loss: 0.2902 - accuracy: 0.9215\n",
      "Epoch 34/100\n",
      " - 136s - loss: 0.2857 - accuracy: 0.9208\n",
      "Epoch 35/100\n",
      " - 139s - loss: 0.2918 - accuracy: 0.9174\n",
      "Epoch 36/100\n",
      " - 140s - loss: 0.2749 - accuracy: 0.9255\n",
      "Epoch 37/100\n",
      " - 138s - loss: 0.2797 - accuracy: 0.9211\n",
      "Epoch 38/100\n",
      " - 141s - loss: 0.2675 - accuracy: 0.9260\n",
      "Epoch 39/100\n",
      " - 139s - loss: 0.2637 - accuracy: 0.9281\n",
      "Epoch 40/100\n",
      " - 141s - loss: 0.2692 - accuracy: 0.9232\n",
      "Epoch 41/100\n",
      " - 118s - loss: 0.2495 - accuracy: 0.9299\n",
      "Epoch 42/100\n",
      " - 127s - loss: 0.2634 - accuracy: 0.9245\n",
      "Epoch 43/100\n",
      " - 121s - loss: 0.2428 - accuracy: 0.9336\n",
      "Epoch 44/100\n",
      " - 114s - loss: 0.2409 - accuracy: 0.9337\n",
      "Epoch 45/100\n",
      " - 110s - loss: 0.2422 - accuracy: 0.9325\n",
      "Epoch 46/100\n",
      " - 109s - loss: 0.2222 - accuracy: 0.9404\n",
      "Epoch 47/100\n",
      " - 111s - loss: 0.2373 - accuracy: 0.9347\n",
      "Epoch 48/100\n",
      " - 125s - loss: 0.2245 - accuracy: 0.9383\n",
      "Epoch 49/100\n",
      " - 160s - loss: 0.2294 - accuracy: 0.9379\n",
      "Epoch 50/100\n",
      " - 157s - loss: 0.2277 - accuracy: 0.9375\n",
      "Epoch 51/100\n",
      " - 140s - loss: 0.2104 - accuracy: 0.9427\n",
      "Epoch 52/100\n",
      " - 143s - loss: 0.2060 - accuracy: 0.9451\n",
      "Epoch 53/100\n",
      " - 142s - loss: 0.2248 - accuracy: 0.9349\n",
      "Epoch 54/100\n",
      " - 143s - loss: 0.2110 - accuracy: 0.9401\n",
      "Epoch 55/100\n",
      " - 147s - loss: 0.1901 - accuracy: 0.9503\n",
      "Epoch 56/100\n",
      " - 145s - loss: 0.2081 - accuracy: 0.9416\n",
      "Epoch 57/100\n",
      " - 155s - loss: 0.1886 - accuracy: 0.9502\n",
      "Epoch 58/100\n",
      " - 141s - loss: 0.1867 - accuracy: 0.9504\n",
      "Epoch 59/100\n",
      " - 142s - loss: 0.2169 - accuracy: 0.9372\n",
      "Epoch 60/100\n",
      " - 142s - loss: 0.1831 - accuracy: 0.9516\n",
      "Epoch 61/100\n",
      " - 141s - loss: 0.1833 - accuracy: 0.9507\n",
      "Epoch 62/100\n",
      " - 141s - loss: 0.1933 - accuracy: 0.9450\n",
      "Epoch 63/100\n",
      " - 139s - loss: 0.1887 - accuracy: 0.9471\n",
      "Epoch 64/100\n",
      " - 140s - loss: 0.1837 - accuracy: 0.9490\n",
      "Epoch 65/100\n",
      " - 148s - loss: 0.1753 - accuracy: 0.9525\n",
      "Epoch 66/100\n",
      " - 139s - loss: 0.1833 - accuracy: 0.9500\n",
      "Epoch 67/100\n",
      " - 158s - loss: 0.2130 - accuracy: 0.9359\n",
      "Epoch 68/100\n",
      " - 174s - loss: 0.1737 - accuracy: 0.9519\n",
      "Epoch 69/100\n",
      " - 172s - loss: 0.1706 - accuracy: 0.9528\n",
      "Epoch 70/100\n",
      " - 168s - loss: 0.1566 - accuracy: 0.9579\n",
      "Epoch 71/100\n",
      " - 141s - loss: 0.1823 - accuracy: 0.9475\n",
      "Epoch 72/100\n",
      " - 147s - loss: 0.1830 - accuracy: 0.9468\n",
      "Epoch 73/100\n",
      " - 139s - loss: 0.1460 - accuracy: 0.9622\n",
      "Epoch 74/100\n",
      " - 140s - loss: 0.1786 - accuracy: 0.9489\n",
      "Epoch 75/100\n",
      " - 138s - loss: 0.1595 - accuracy: 0.9567\n",
      "Epoch 76/100\n",
      " - 137s - loss: 0.1623 - accuracy: 0.9536\n",
      "Epoch 77/100\n",
      " - 137s - loss: 0.1749 - accuracy: 0.9493\n",
      "Epoch 78/100\n",
      " - 140s - loss: 0.1515 - accuracy: 0.9598\n",
      "Epoch 79/100\n",
      " - 140s - loss: 0.1694 - accuracy: 0.9529\n",
      "Epoch 80/100\n",
      " - 146s - loss: 0.1420 - accuracy: 0.9629\n",
      "Epoch 81/100\n",
      " - 138s - loss: 0.1426 - accuracy: 0.9611\n",
      "Epoch 82/100\n",
      " - 140s - loss: 0.1777 - accuracy: 0.9482\n",
      "Epoch 83/100\n",
      " - 137s - loss: 0.1386 - accuracy: 0.9630\n",
      "Epoch 84/100\n",
      " - 142s - loss: 0.1651 - accuracy: 0.9515\n",
      "Epoch 85/100\n",
      " - 141s - loss: 0.1809 - accuracy: 0.9464\n",
      "Epoch 86/100\n",
      " - 140s - loss: 0.1405 - accuracy: 0.9617\n",
      "Epoch 87/100\n",
      " - 141s - loss: 0.1332 - accuracy: 0.9643\n",
      "Epoch 88/100\n",
      " - 141s - loss: 0.1665 - accuracy: 0.9509\n",
      "Epoch 89/100\n",
      " - 138s - loss: 0.1546 - accuracy: 0.9562\n",
      "Epoch 90/100\n",
      " - 141s - loss: 0.1401 - accuracy: 0.9607\n",
      "Epoch 91/100\n",
      " - 141s - loss: 0.1333 - accuracy: 0.9641\n",
      "Epoch 92/100\n",
      " - 141s - loss: 0.1655 - accuracy: 0.9496\n",
      "Epoch 93/100\n",
      " - 142s - loss: 0.1583 - accuracy: 0.9542\n",
      "Epoch 94/100\n",
      " - 146s - loss: 0.1178 - accuracy: 0.9708\n",
      "Epoch 95/100\n",
      " - 144s - loss: 0.1456 - accuracy: 0.9588\n",
      "Epoch 96/100\n",
      " - 144s - loss: 0.1637 - accuracy: 0.9512\n",
      "Epoch 97/100\n",
      " - 146s - loss: 0.1474 - accuracy: 0.9583\n",
      "Epoch 98/100\n",
      " - 137s - loss: 0.1358 - accuracy: 0.9620\n",
      "Epoch 99/100\n",
      " - 142s - loss: 0.1380 - accuracy: 0.9616\n",
      "Epoch 100/100\n",
      " - 192s - loss: 0.1306 - accuracy: 0.9634\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x252ffc95748>"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train model on sentences for 100 epochs\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "model.fit(X, y, batch_size = 32, epochs=100, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save model\n",
    "model.save('model_linebreak.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function from https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py, samples index from probability array with particular temperature\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to generate a 14 line sonnet with a particular temperature\n",
    "\n",
    "def generate_sonnet(seed, temperature, model, mapping, vocab_size):\n",
    "    \n",
    "    print('\\n')\n",
    "    print('Generating sonnet with temperature:', temperature,'\\n')\n",
    "    #Start sonnet and print it out\n",
    "    sonnet = seed\n",
    "    sys.stdout.write(sonnet)\n",
    "    \n",
    "    #Encode seed\n",
    "    encode = [mapping[char] for char in seed]\n",
    "    sequence = np.array([to_categorical(x, num_classes=vocab_size) for x in encode])\n",
    "    \n",
    "    #Write out 14 lines\n",
    "    lines = 1\n",
    "    while lines < 14:\n",
    "        \n",
    "        #Predict and write the next character\n",
    "        pred = model.predict(np.array([sequence]))\n",
    "        next_enc = sample(pred[0], temperature = temperature)\n",
    "        next_char = indices_char[next_enc]\n",
    "        sys.stdout.write(next_char)\n",
    "        \n",
    "        #Add next character to sequence\n",
    "        sonnet = sonnet + next_char\n",
    "        sequence = np.append(sequence[1:,:], np.array([to_categorical(next_enc,num_classes = vocab_size)]), axis = 0)\n",
    "        \n",
    "        #Update the lines\n",
    "        if next_char == '\\n':\n",
    "            lines = lines + 1\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Generating sonnet with temperature: 1.5 \n",
      "\n",
      "shall i compare thee to a summer's day?\n",
      "Why sho shange at seeelide of my argure the stare\n",
      "While by bornions brest in one rust.\n",
      "And thy self could in their forle-thine own,\n",
      "  And unquest and admenting ad und such ridem\n",
      "Pascuse on dead fal withou, 'rournisured,\n",
      "Leather that outwill love's bewell of propence:\n",
      "In wind'st can pussome hear the love.\n",
      "  Whise id dustile ars auspapit mid your\n",
      "And my noghtan diving Timp'ss eyes with dut make me mort,\n",
      "  Wist to his swite, uplad thy love,\n",
      "Where and you are had the word of renst heart\n",
      "Which fabbtes it,at cally discrigit:\n",
      "So thould the branty, when whal obeaun'se:\n",
      "\n",
      "\n",
      "Generating sonnet with temperature: 0.75 \n",
      "\n",
      "shall i compare thee to a summer's day?\n",
      "Why sho love swiends the fire in one rud.\n",
      "Then im the love's be things therefteer act,\n",
      "Eacusted than thou his sweet fees be undee,\n",
      "They hearth is receith my awastes ime.\n",
      "  'stakn anveng was sprop you is my love,\n",
      "As thy sond of you your sweet see those shall,\n",
      "  And away, with my migut no when in my heart,\n",
      "Whir the benobress af ceest hath to thee die forcht,\n",
      "When thy self for becries so sloodyst ford,\n",
      "For that be self that men bo bew-tuen be bling.\n",
      "  But agringe crainst shoutes it is of thy bron,\n",
      "The rend inwolt is a tires ay's trust yst be,\n",
      "As the dear be in of art thy revenmed:\n",
      "\n",
      "\n",
      "Generating sonnet with temperature: 0.25 \n",
      "\n",
      "shall i compare thee to a summer's day?\n",
      "Why sho longer lips vising parts make my manter,\n",
      "The where to my misures but thy suggedion;\n",
      "Alt farch ot dooks, of love to come and stay:\n",
      "Which by songufer I sinfle like's sight is rong.\n",
      "For you art hange is muth to betten then leaved:\n",
      "To barce one outwhes thy torl forgot for hyove.\n",
      "  Leet the world, of flarse of me, for the cay,\n",
      "  So it I coll than be wong, and moares deemment,\n",
      "Sised from the wornding, where a tomen;\n",
      "Soprif love defor my luve a love,\n",
      "Comp ar ne, and hearts discrisgle is reast.\n",
      "For that this shy love, that fartule in re,\n",
      "Whose with thy mind's with yet see love you subst,\n"
     ]
    }
   ],
   "source": [
    "#Generate a sonnet for each of the temperatures\n",
    "for temp in [1.5,0.75,0.25]:\n",
    "    generate_sonnet(\"shall i compare thee to a summer's day?\\n\", temp, model, mapping, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Generating sonnet with temperature: 1.5 \n",
      "\n",
      "shall i compare thee to a summer's day?\n",
      "Why sho long amoun hast if tlat the dear prige:\n",
      "Whther whal bety is that right of ender knows do prove.\n",
      "For mare te frest her up michsed conce, now.\n",
      "  Beauga rew ence, and loth with leass knows,\n",
      "Toou of your for my sake ley love doopss:\n",
      "But of the prassed confan books didi.\n",
      "How with thy propers I desing worthe wond.\n",
      "For you upraition and by the givent,\n",
      "Make the cradiey in the bladues ame,\n",
      "  Hasce I (oroutused than we I chen,\n",
      "While it ferttily, Tul him that with up sweet\n",
      "Give and therievent, nor to be befally:\n",
      "  For where thy sweet fires not shad with sweet;\n",
      "\n",
      "\n",
      "Generating sonnet with temperature: 0.75 \n",
      "\n",
      "shall i compare thee to a summer's day?\n",
      "Why sho sonf art awad your love the livers\n",
      "  You gaid that menus of this his a trove\n",
      "Hup, wil my soul by therefoued) not gookers:\n",
      "Hawe and that les, with unsedier encet\n",
      "Thee, marue mookerms thee with thou wist,\n",
      "  And ecquercance I it math awad on the rus's,\n",
      "O where and thy love, and she wat my swall still,\n",
      "Whal his nut own showliduly in the rair will:\n",
      "Thee wordneds of your for thinks, one thine owarty sight,\n",
      "  But shalt warch aboermers and deathess quill\n",
      "To whisk is a tith're but and loaves,\n",
      "Thy forld bay, where that for my love aboth,\n",
      "  If thee I latt, to fold looke I dweds show,\n",
      "\n",
      "\n",
      "Generating sonnet with temperature: 0.25 \n",
      "\n",
      "shall i compare thee to a summer's day?\n",
      "Why sho longer lisg thy song of your\n",
      "Hut this my friess the time and than tell;\n",
      "Boaking all those poor must net recad, wee.\n",
      "For that be my happy beaute with then love'.\n",
      "  He noth, all my so fatures it'r in eyest,\n",
      "When I self, in dust to his grown your due,\n",
      "And thee despich dost can that thinks gaze,\n",
      "Whisef'll is aut they los, will for my swall,\n",
      "Than teave amagote with unsubon a me prove?\n",
      "And swarty'st can that thin seie me slow redimmerse?\n",
      "To pright to well the cunsern with blast,\n",
      "Where you ore ungowed leasures go dake thee,\n",
      "Thute invering the trum, with felfoe reaut,\n"
     ]
    }
   ],
   "source": [
    "#Generate a sonnet for each of the temperatures\n",
    "for temp in [1.5,0.75,0.25]:\n",
    "    generate_sonnet(\"shall i compare thee to a summer's day?\\n\", temp, model, mapping, vocab_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
